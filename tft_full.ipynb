{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60438412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d7e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows total: 159180\n",
      "Periode min/max: 2021-01-01 00:00:00 -> 2024-10-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(r\"D:\\C14220255\\tft_dataset_full_eligible.csv\")\n",
    "OUT_DIR   = Path(r\"D:\\C14220255\\outputs_tft_full\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(\n",
    "    DATA_PATH,\n",
    "    parse_dates=[\"periode\"],\n",
    "    dtype={\"area\": str},    \n",
    "    low_memory=False          \n",
    ")\n",
    "\n",
    "df = df.sort_values([\"cabang\",\"sku\",\"periode\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Rows total:\", len(df))\n",
    "print(\"Periode min/max:\", df[\"periode\"].min(), \"->\", df[\"periode\"].max())\n",
    "\n",
    "req_cols = [\"cabang\",\"sku\",\"periode\",\"qty\",\"is_train\",\"is_test\",\"sample_weight\"]\n",
    "miss = [c for c in req_cols if c not in df.columns]\n",
    "if miss:\n",
    "    raise ValueError(f\"Kolom wajib hilang di dataset: {miss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa47db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved series_info_full: D:\\C14220255\\series_info_full_tft.csv\n",
      "Total seri: 3898 | eligible_tft: 3898\n"
     ]
    }
   ],
   "source": [
    "TRAIN_END = pd.Timestamp(\"2024-05-01\")\n",
    "\n",
    "if \"is_train\" not in df.columns or \"is_test\" not in df.columns:\n",
    "    raise ValueError(\"Kolom is_train / is_test tidak ada di dataset. Cek CSV dulu.\")\n",
    "\n",
    "# pastikan urutan\n",
    "df = df.sort_values([\"cabang\", \"sku\", \"periode\"]).reset_index(drop=True)\n",
    "\n",
    "# hitung info seri hanya dari periode sampai TRAIN_END\n",
    "base = df[df[\"periode\"] <= TRAIN_END].copy()\n",
    "\n",
    "grp = base.groupby([\"cabang\", \"sku\"], as_index=False)\n",
    "\n",
    "info = grp.agg(\n",
    "    n_months=(\"qty\", \"size\"),\n",
    "    nonzero_months=(\"qty\", lambda s: (s > 0).sum()),\n",
    "    total_qty=(\"qty\", \"sum\"),\n",
    ")\n",
    "\n",
    "# zero_ratio_train dan n_train pakai flag is_train\n",
    "train_part = df[df[\"is_train\"] == 1].copy()\n",
    "gtr = train_part.groupby([\"cabang\", \"sku\"])\n",
    "\n",
    "zr = gtr[\"qty\"].apply(lambda s: (s == 0).mean()).reset_index(name=\"zero_ratio_train\")\n",
    "ntr = gtr[\"qty\"].count().reset_index(name=\"n_train\")\n",
    "\n",
    "# qty 12 bulan terakhir di train\n",
    "last12 = (\n",
    "    train_part.sort_values([\"cabang\", \"sku\", \"periode\"])\n",
    "    .groupby([\"cabang\", \"sku\"], as_index=False)\n",
    "    .agg(qty_12m=(\"qty\", lambda s: s.tail(12).sum()))\n",
    ")\n",
    "\n",
    "# last non zero di train\n",
    "nz = (\n",
    "    train_part[train_part[\"qty\"] > 0]\n",
    "    .groupby([\"cabang\", \"sku\"], as_index=False)[\"periode\"]\n",
    "    .max()\n",
    "    .rename(columns={\"periode\": \"last_nz\"})\n",
    ")\n",
    "\n",
    "info = (\n",
    "    info.merge(zr, on=[\"cabang\", \"sku\"], how=\"left\")\n",
    "        .merge(ntr, on=[\"cabang\", \"sku\"], how=\"left\")\n",
    "        .merge(last12, on=[\"cabang\", \"sku\"], how=\"left\")\n",
    "        .merge(nz, on=[\"cabang\", \"sku\"], how=\"left\")\n",
    ")\n",
    "\n",
    "# isi NA\n",
    "for c in [\"zero_ratio_train\", \"n_train\", \"qty_12m\"]:\n",
    "    info[c] = info[c].fillna(0)\n",
    "\n",
    "info[\"last_nz\"] = pd.to_datetime(info[\"last_nz\"], errors=\"coerce\")\n",
    "\n",
    "train_end_per = TRAIN_END.to_period(\"M\")\n",
    "info[\"months_since_last_nz\"] = 999\n",
    "\n",
    "mask = info[\"last_nz\"].notna()\n",
    "if mask.any():\n",
    "    last_nz_per = info.loc[mask, \"last_nz\"].dt.to_period(\"M\")\n",
    "    diff = train_end_per.ordinal - last_nz_per.astype(\"int64\")\n",
    "    info.loc[mask, \"months_since_last_nz\"] = diff.values\n",
    "\n",
    "info[\"months_since_last_nz\"] = info[\"months_since_last_nz\"].astype(int)\n",
    "\n",
    "# alive_recent: ada aktivitas 3 bulan terakhir\n",
    "info[\"alive_recent\"] = (\n",
    "    (info[\"qty_12m\"] > 0) &\n",
    "    (info[\"months_since_last_nz\"] <= 3)\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "info[\"eligible_tft\"] = (\n",
    "    (info[\"n_months\"] >= 30) &          # cukup panjang\n",
    "    (info[\"nonzero_months\"] >= 10) &    # tidak terlalu sparse\n",
    "    (info[\"total_qty\"] >= 30) &         # bukan seri sampah\n",
    "    (info[\"zero_ratio_train\"] <= 0.7) & # tidak 90% nol\n",
    "    (info[\"alive_recent\"] == 1)         # masih \"hidup\"\n",
    ").astype(int)\n",
    "\n",
    "SERIES_INFO_PATH = r\"D:\\C14220255\\series_info_full_tft.csv\"\n",
    "info.to_csv(SERIES_INFO_PATH, index=False)\n",
    "print(\"Saved series_info_full:\", SERIES_INFO_PATH)\n",
    "\n",
    "print(\"Total seri:\", len(info), \"| eligible_tft:\", info[\"eligible_tft\"].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56312a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after TFT filter: 159180\n",
      "Unique series: 3898\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = r\"D:\\C14220255\\tft_dataset_full_eligible.csv\"\n",
    "\n",
    "df = pd.read_csv(\n",
    "    DATA_PATH,\n",
    "    parse_dates=[\"periode\"],\n",
    "    dtype={\"area\": str},\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "df = df.sort_values([\"cabang\", \"sku\", \"periode\"]).reset_index(drop=True)\n",
    "\n",
    "# merge eligible_tft\n",
    "info = pd.read_csv(r\"D:\\C14220255\\series_info_full_tft.csv\")\n",
    "\n",
    "df = df.merge(\n",
    "    info[[\"cabang\", \"sku\", \"eligible_tft\"]],\n",
    "    on=[\"cabang\", \"sku\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df[\"eligible_tft\"] = df[\"eligible_tft\"].fillna(0).astype(int)\n",
    "\n",
    "# filter: pakai semua yang eligible (sekarang = semua)\n",
    "df = df[df[\"eligible_tft\"] == 1].copy()\n",
    "df = df.sort_values([\"cabang\", \"sku\", \"periode\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Rows after TFT filter:\", len(df))\n",
    "print(\"Unique series:\", df[[\"cabang\", \"sku\"]].drop_duplicates().shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b71b93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows total: 159180\n",
      "Periode min/max: 2021-01-01 00:00:00 -> 2024-10-01 00:00:00\n",
      "Rows after TFT filter: 159180\n",
      "Unique series: 3898\n",
      "Sisa NA setelah cleaning:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "DATA_PATH = r\"D:\\C14220255\\tft_dataset_full_eligible.csv\"\n",
    "INFO_PATH = r\"D:\\C14220255\\series_info_full_tft.csv\"\n",
    "OUT_DIR   = Path(r\"D:\\C14220255\\tft_out_full_global\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(\n",
    "    DATA_PATH,\n",
    "    parse_dates=[\"periode\"],\n",
    "    dtype={\"area\": str},\n",
    "    low_memory=False,\n",
    ")\n",
    "\n",
    "print(\"Rows total:\", len(df))\n",
    "print(\"Periode min/max:\", df[\"periode\"].min(), \"->\", df[\"periode\"].max())\n",
    "\n",
    "info = pd.read_csv(INFO_PATH)\n",
    "\n",
    "df = df.merge(\n",
    "    info[[\"cabang\", \"sku\", \"eligible_tft\"]],\n",
    "    on=[\"cabang\", \"sku\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df[\"eligible_tft\"] = df[\"eligible_tft\"].fillna(0).astype(int)\n",
    "\n",
    "# Hanya pakai seri eligible (sekarang 3898 semua)\n",
    "df = df[df[\"eligible_tft\"] == 1].copy()\n",
    "df = df.sort_values([\"cabang\", \"sku\", \"periode\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Rows after TFT filter:\", len(df))\n",
    "print(\"Unique series:\", df[[\"cabang\", \"sku\"]].drop_duplicates().shape[0])\n",
    "\n",
    "df[\"qty\"] = df[\"qty\"].clip(lower=0)\n",
    "df[\"qty_log\"] = np.log1p(df[\"qty\"])\n",
    "\n",
    "if \"spike_flag\" not in df.columns:\n",
    "    df[\"spike_flag\"] = 0\n",
    "\n",
    "df[\"w_tft\"] = 1.0\n",
    "df.loc[df[\"spike_flag\"] == 1, \"w_tft\"] = 3.0  \n",
    "\n",
    "\n",
    "# TIME_IDX & LAG/ROLL CLEANING\n",
    "\n",
    "df = df.sort_values([\"cabang\", \"sku\", \"periode\"]).reset_index(drop=True)\n",
    "df[\"time_idx\"] = df.groupby([\"cabang\", \"sku\"]).cumcount()\n",
    "\n",
    "rolling_cols = [\n",
    "    \"qty_rollmean_3\",\"qty_rollstd_3\",\n",
    "    \"qty_rollmean_6\",\"qty_rollstd_6\",\n",
    "    \"qty_rollmean_12\",\"qty_rollstd_12\",\n",
    "]\n",
    "rolling_cols = [c for c in rolling_cols if c in df.columns]\n",
    "\n",
    "lag_cols = [c for c in df.columns if c.startswith(\"qty_lag\")]\n",
    "\n",
    "# rolling -> bfill/ffill per seri\n",
    "if rolling_cols:\n",
    "    df[rolling_cols] = (\n",
    "        df.groupby([\"cabang\",\"sku\"])[rolling_cols]\n",
    "          .transform(lambda g: g.bfill().ffill())\n",
    "    )\n",
    "\n",
    "# lag -> NaN jadi 0\n",
    "if lag_cols:\n",
    "    df[lag_cols] = df[lag_cols].fillna(0)\n",
    "\n",
    "for col in [\"event_flag_lag1\",\"holiday_count_lag1\",\"rainfall_lag1\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "print(\"Sisa NA setelah cleaning:\")\n",
    "print(df.isna().sum()[df.isna().sum() > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690a983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baris train: 159135\n",
      "Range time_idx train: 0 -> 40\n",
      "training_cutoff: 34\n",
      "Jumlah sample train: 69481\n",
      "Jumlah sample val  : 69481\n"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "\n",
    "# TRAIN SPLIT\n",
    "train_df = df[df[\"is_train\"] == 1].copy()\n",
    "\n",
    "print(\"Baris train:\", len(train_df))\n",
    "print(\"Range time_idx train:\", train_df[\"time_idx\"].min(), \"->\", train_df[\"time_idx\"].max())\n",
    "\n",
    "# 6 bulan terakhir sebagai validasi internal\n",
    "training_cutoff = train_df[\"time_idx\"].max() - 6\n",
    "print(\"training_cutoff:\", training_cutoff)\n",
    "\n",
    "# KONFIG DATASET\n",
    "static_cat = [\"cabang\", \"sku\"]\n",
    "\n",
    "known_reals = [\n",
    "    \"time_idx\",\n",
    "    \"event_flag\", \"event_flag_lag1\",\n",
    "    \"holiday_count\", \"holiday_count_lag1\",\n",
    "    \"rainfall_lag1\",\n",
    "    \"spike_flag\",\n",
    "]\n",
    "known_reals = [c for c in known_reals if c in df.columns]\n",
    "\n",
    "unknown_reals = [\"qty_log\"] + rolling_cols + lag_cols\n",
    "\n",
    "min_encoder_length = 12\n",
    "max_encoder_length = 24\n",
    "max_prediction_length = 1\n",
    "\n",
    "training_ds = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"qty_log\",\n",
    "    group_ids=[\"cabang\", \"sku\"],\n",
    "    weight=\"w_tft\",\n",
    "\n",
    "    min_encoder_length=min_encoder_length,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "\n",
    "    static_categoricals=static_cat,\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=[],\n",
    "    time_varying_known_reals=known_reals,\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=unknown_reals,\n",
    "\n",
    "    target_normalizer=GroupNormalizer(groups=[\"cabang\",\"sku\"]),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "\n",
    "    # ini yang bikin split train/val internal\n",
    "    min_prediction_idx=training_cutoff + 1,\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = training_ds.to_dataloader(\n",
    "    train=True,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = training_ds.to_dataloader(\n",
    "    train=False,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(\"Jumlah sample train:\", len(train_loader.dataset))\n",
    "print(\"Jumlah sample val  :\", len(val_loader.dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "467e8b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah seri yang punya periode test: 9\n",
      "  cabang          sku  n_test\n",
      "0    02A   BUVW001KSW       5\n",
      "1    05A   BUVW001KSW       5\n",
      "2    13A  DOPQ001K002       5\n",
      "3    13I   BUVW001KSW       5\n",
      "4    14A   BUVW001KSW       5\n",
      "5    16C  DOPQ001K009       5\n",
      "6    17A  DOPQ001K002       5\n",
      "7    23A   BUVW001KSW       5\n",
      "8    29A   BUVW001KSW       5\n"
     ]
    }
   ],
   "source": [
    "# cek berapa seri yg punya is_test == 1\n",
    "test_series = (\n",
    "    df[df[\"is_test\"] == 1]\n",
    "      .groupby([\"cabang\",\"sku\"])\n",
    "      .size()\n",
    "      .reset_index(name=\"n_test\")\n",
    ")\n",
    "\n",
    "print(\"Jumlah seri yang punya periode test:\", len(test_series))\n",
    "print(test_series.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d1038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "d:\\C14220255\\.venv\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "d:\\C14220255\\.venv\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RS Trial 1/12 ===\n",
      "{'hidden_size': 16, 'lstm_layers': 1, 'dropout': 0.24831009995196657, 'attention_head_size': 1, 'learning_rate': 0.0009026689930018215}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "d:\\C14220255\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 16.6 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.0 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 19.3 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 5.1 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 17     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "57.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "57.1 K    Total params\n",
      "0.228     Total estimated model params size (MB)\n",
      "726       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\C14220255\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "d:\\C14220255\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:15<00:00,  4.24it/s, v_num=52, train_loss_step=0.315, val_loss=0.274, train_loss_epoch=0.304]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:16<00:00,  4.24it/s, v_num=52, train_loss_step=0.315, val_loss=0.274, train_loss_epoch=0.304]\n",
      "val_loss = 0.2739377021789551\n",
      "\n",
      "=== RS Trial 2/12 ===\n",
      "{'hidden_size': 16, 'lstm_layers': 1, 'dropout': 0.21809850248980794, 'attention_head_size': 1, 'learning_rate': 0.0003804524924827899}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 16.6 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.0 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 19.3 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 5.1 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 17     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "57.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "57.1 K    Total params\n",
      "0.228     Total estimated model params size (MB)\n",
      "726       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:09<00:00,  4.34it/s, v_num=53, train_loss_step=0.297, val_loss=0.346, train_loss_epoch=0.375]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:10<00:00,  4.34it/s, v_num=53, train_loss_step=0.297, val_loss=0.346, train_loss_epoch=0.375]\n",
      "val_loss = 0.3461090624332428\n",
      "\n",
      "=== RS Trial 3/12 ===\n",
      "{'hidden_size': 24, 'lstm_layers': 1, 'dropout': 0.20107105762067246, 'attention_head_size': 1, 'learning_rate': 0.0018153616699342551}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 24.9 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.7 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 27.1 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 6.7 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 2.4 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 2.4 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 2.4 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 2.4 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 4.8 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 4.8 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 1.2 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 48     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 3.0 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 2.4 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 1.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 2.4 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 1.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 25     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "92.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "92.2 K    Total params\n",
      "0.369     Total estimated model params size (MB)\n",
      "726       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:25<00:00,  4.09it/s, v_num=54, train_loss_step=0.207, val_loss=0.219, train_loss_epoch=0.241]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:25<00:00,  4.08it/s, v_num=54, train_loss_step=0.207, val_loss=0.219, train_loss_epoch=0.241]\n",
      "val_loss = 0.2189732939004898\n",
      "\n",
      "=== RS Trial 4/12 ===\n",
      "{'hidden_size': 40, 'lstm_layers': 1, 'dropout': 0.18984180925677072, 'attention_head_size': 2, 'learning_rate': 0.002485462233030132}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 41.5 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 38.5 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.7 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 6.6 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 6.6 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 6.6 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 6.6 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 13.1 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 13.1 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.3 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 80     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 8.2 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.9 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.4 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 6.6 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.4 K  | train\n",
      "20 | output_layer                       | Linear                          | 41     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "176 K     Trainable params\n",
      "0         Non-trainable params\n",
      "176 K     Total params\n",
      "0.705     Total estimated model params size (MB)\n",
      "728       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:46<00:00,  3.78it/s, v_num=55, train_loss_step=0.131, val_loss=0.226, train_loss_epoch=0.237]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:46<00:00,  3.78it/s, v_num=55, train_loss_step=0.131, val_loss=0.226, train_loss_epoch=0.237]\n",
      "val_loss = 0.22603189945220947\n",
      "\n",
      "=== RS Trial 5/12 ===\n",
      "{'hidden_size': 16, 'lstm_layers': 1, 'dropout': 0.23962787899764537, 'attention_head_size': 2, 'learning_rate': 0.001050252622513433}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 16.6 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.0 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 19.3 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 5.1 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 808    | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 17     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "56.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "56.9 K    Total params\n",
      "0.227     Total estimated model params size (MB)\n",
      "728       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:10<00:00,  4.34it/s, v_num=56, train_loss_step=0.333, val_loss=0.250, train_loss_epoch=0.283]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:10<00:00,  4.33it/s, v_num=56, train_loss_step=0.333, val_loss=0.250, train_loss_epoch=0.283]\n",
      "val_loss = 0.24986065924167633\n",
      "\n",
      "=== RS Trial 6/12 ===\n",
      "{'hidden_size': 24, 'lstm_layers': 2, 'dropout': 0.12044205530396974, 'attention_head_size': 2, 'learning_rate': 0.0005611342174503529}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 24.9 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.7 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 27.1 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 6.7 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 2.4 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 2.4 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 2.4 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 2.4 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 9.6 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 9.6 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 1.2 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 48     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 3.0 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.8 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 1.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 2.4 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 1.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 25     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.405     Total estimated model params size (MB)\n",
      "728       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:42<00:00,  3.84it/s, v_num=57, train_loss_step=0.297, val_loss=0.227, train_loss_epoch=0.267]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:42<00:00,  3.84it/s, v_num=57, train_loss_step=0.297, val_loss=0.227, train_loss_epoch=0.267]\n",
      "val_loss = 0.22685879468917847\n",
      "\n",
      "=== RS Trial 7/12 ===\n",
      "{'hidden_size': 32, 'lstm_layers': 2, 'dropout': 0.261425654654876, 'attention_head_size': 4, 'learning_rate': 0.0015404471751139749}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 33.2 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.3 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 33.3 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 8.2 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 16.9 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 16.9 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 2.6 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 33     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "147 K     Trainable params\n",
      "0         Non-trainable params\n",
      "147 K     Total params\n",
      "0.591     Total estimated model params size (MB)\n",
      "732       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:45<00:00,  3.80it/s, v_num=58, train_loss_step=0.132, val_loss=0.209, train_loss_epoch=0.236]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:45<00:00,  3.80it/s, v_num=58, train_loss_step=0.132, val_loss=0.209, train_loss_epoch=0.236]\n",
      "val_loss = 0.2094854712486267\n",
      "\n",
      "=== RS Trial 8/12 ===\n",
      "{'hidden_size': 16, 'lstm_layers': 2, 'dropout': 0.11576003961569165, 'attention_head_size': 2, 'learning_rate': 0.002539392593483086}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 16.6 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.0 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 19.3 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 5.1 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 4.4 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 4.4 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 808    | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 17     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "61.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "61.2 K    Total params\n",
      "0.245     Total estimated model params size (MB)\n",
      "728       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:21<00:00,  4.15it/s, v_num=59, train_loss_step=0.211, val_loss=0.216, train_loss_epoch=0.240]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:21<00:00,  4.15it/s, v_num=59, train_loss_step=0.211, val_loss=0.216, train_loss_epoch=0.240]\n",
      "val_loss = 0.21580371260643005\n",
      "\n",
      "=== RS Trial 9/12 ===\n",
      "{'hidden_size': 32, 'lstm_layers': 1, 'dropout': 0.24091436724298468, 'attention_head_size': 1, 'learning_rate': 0.0020854109601328177}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 33.2 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.3 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 33.3 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 8.2 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 8.4 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 8.4 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.2 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 33     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "132 K     Trainable params\n",
      "0         Non-trainable params\n",
      "132 K     Total params\n",
      "0.529     Total estimated model params size (MB)\n",
      "726       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:48<00:00,  3.76it/s, v_num=60, train_loss_step=0.238, val_loss=0.225, train_loss_epoch=0.241]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:48<00:00,  3.76it/s, v_num=60, train_loss_step=0.238, val_loss=0.225, train_loss_epoch=0.241]\n",
      "val_loss = 0.22464647889137268\n",
      "\n",
      "=== RS Trial 10/12 ===\n",
      "{'hidden_size': 32, 'lstm_layers': 1, 'dropout': 0.27106354420302936, 'attention_head_size': 1, 'learning_rate': 0.0013263408076057486}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 33.2 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.3 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 33.3 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 8.2 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 8.4 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 8.4 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.2 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 33     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "132 K     Trainable params\n",
      "0         Non-trainable params\n",
      "132 K     Total params\n",
      "0.529     Total estimated model params size (MB)\n",
      "726       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:29<00:00,  4.02it/s, v_num=61, train_loss_step=0.275, val_loss=0.211, train_loss_epoch=0.234]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:29<00:00,  4.02it/s, v_num=61, train_loss_step=0.275, val_loss=0.211, train_loss_epoch=0.234]\n",
      "val_loss = 0.2114083617925644\n",
      "\n",
      "=== RS Trial 11/12 ===\n",
      "{'hidden_size': 40, 'lstm_layers': 2, 'dropout': 0.13253081943121697, 'attention_head_size': 2, 'learning_rate': 0.0008656689830830167}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 41.5 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 38.5 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.7 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 6.6 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 6.6 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 6.6 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 6.6 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 26.2 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 26.2 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.3 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 80     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 8.2 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.9 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.4 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 6.6 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.4 K  | train\n",
      "20 | output_layer                       | Linear                          | 41     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "202 K     Trainable params\n",
      "0         Non-trainable params\n",
      "202 K     Total params\n",
      "0.810     Total estimated model params size (MB)\n",
      "728       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [05:04<00:00,  3.56it/s, v_num=62, train_loss_step=0.239, val_loss=0.190, train_loss_epoch=0.217]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [05:05<00:00,  3.56it/s, v_num=62, train_loss_step=0.239, val_loss=0.190, train_loss_epoch=0.217]\n",
      "val_loss = 0.19012488424777985\n",
      "\n",
      "=== RS Trial 12/12 ===\n",
      "{'hidden_size': 32, 'lstm_layers': 1, 'dropout': 0.22182620113339763, 'attention_head_size': 1, 'learning_rate': 0.0017421776679802116}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 33.2 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.3 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 33.3 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 8.2 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 8.4 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 8.4 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.2 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 33     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "132 K     Trainable params\n",
      "0         Non-trainable params\n",
      "132 K     Total params\n",
      "0.529     Total estimated model params size (MB)\n",
      "726       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:28<00:00,  4.04it/s, v_num=63, train_loss_step=0.209, val_loss=0.234, train_loss_epoch=0.257]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:28<00:00,  4.04it/s, v_num=63, train_loss_step=0.209, val_loss=0.234, train_loss_epoch=0.257]\n",
      "val_loss = 0.2343195378780365\n",
      "\n",
      "Best RS Params: {'hidden_size': 40, 'lstm_layers': 2, 'dropout': 0.13253081943121697, 'attention_head_size': 2, 'learning_rate': 0.0008656689830830167}\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "import random\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "loss_q = QuantileLoss(quantiles=[0.5])\n",
    "\n",
    "def sample_params_rs():\n",
    "    return {\n",
    "        \"hidden_size\": random.choice([16, 24, 32, 40]),\n",
    "        \"lstm_layers\": random.choice([1, 2]),\n",
    "        \"dropout\": random.uniform(0.1, 0.3),\n",
    "        \"attention_head_size\": random.choice([1, 2, 4]),\n",
    "        \"learning_rate\": random.uniform(3e-4, 3e-3),\n",
    "    }\n",
    "\n",
    "N_RS = 12     \n",
    "\n",
    "rs_results = []\n",
    "\n",
    "for i in range(N_RS):\n",
    "    p = sample_params_rs()\n",
    "    print(f\"\\n=== RS Trial {i+1}/{N_RS} ===\")\n",
    "    print(p)\n",
    "\n",
    "    model = TemporalFusionTransformer.from_dataset(\n",
    "        training_ds,\n",
    "        hidden_size=p[\"hidden_size\"],\n",
    "        lstm_layers=p[\"lstm_layers\"],\n",
    "        dropout=p[\"dropout\"],\n",
    "        attention_head_size=p[\"attention_head_size\"],\n",
    "        learning_rate=p[\"learning_rate\"],\n",
    "        loss=loss_q,\n",
    "        output_size=1,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=20,\n",
    "        accelerator=\"cpu\",\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", patience=3)],\n",
    "        enable_progress_bar=True\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    val_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "    \n",
    "    rs_results.append((val_loss, p))\n",
    "    print(\"val_loss =\", val_loss)\n",
    "\n",
    "best_rs = sorted(rs_results, key=lambda x: x[0])[0][1]\n",
    "print(\"\\nBest RS Params:\", best_rs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aab231be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 16:59:51,466] A new study created in memory with name: no-name-6c175c2f-c0fb-4d38-81db-a4cafe42281d\n",
      "d:\\C14220255\\.venv\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "d:\\C14220255\\.venv\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 47.7 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.4 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 42.4 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 10.9 K | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 8.7 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 8.7 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 8.7 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 8.7 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 34.6 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 34.6 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 4.3 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 92     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 10.9 K | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 5.2 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 4.4 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 8.7 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 4.4 K  | train\n",
      "20 | output_layer                       | Linear                          | 47     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "247 K     Trainable params\n",
      "0         Non-trainable params\n",
      "247 K     Total params\n",
      "0.990     Total estimated model params size (MB)\n",
      "732       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "d:\\C14220255\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "d:\\C14220255\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-16 19:09:30,495] Trial 0 finished with value: 0.20063017308712006 and parameters: {'hidden_size': 46, 'lstm_layers': 2, 'dropout': 0.09690653129492553, 'attention_head_size': 4, 'learning_rate': 0.00032190983340975613}. Best is trial 0 with value: 0.20063017308712006.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 38.4 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.7 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 36.6 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.2 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 5.7 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 5.7 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 5.7 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 5.7 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 22.5 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 22.5 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.8 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 74     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 7.1 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.4 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.9 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 5.7 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.9 K  | train\n",
      "20 | output_layer                       | Linear                          | 38     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "180 K     Trainable params\n",
      "0         Non-trainable params\n",
      "180 K     Total params\n",
      "0.721     Total estimated model params size (MB)\n",
      "732       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-16 21:21:02,666] Trial 1 finished with value: 0.34954729676246643 and parameters: {'hidden_size': 37, 'lstm_layers': 2, 'dropout': 0.24477829124089162, 'attention_head_size': 4, 'learning_rate': 0.00012345322937028984}. Best is trial 0 with value: 0.20063017308712006.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 40.5 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 37.9 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.5 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 6.3 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 6.3 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 6.3 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 6.3 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 25.0 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 25.0 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.1 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 78     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 7.8 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.5 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 6.3 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 40     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "195 K     Trainable params\n",
      "0         Non-trainable params\n",
      "195 K     Total params\n",
      "0.781     Total estimated model params size (MB)\n",
      "728       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-16 23:31:21,242] Trial 2 finished with value: 0.23964743316173553 and parameters: {'hidden_size': 39, 'lstm_layers': 2, 'dropout': 0.24179562854875963, 'attention_head_size': 2, 'learning_rate': 0.00039106831786499155}. Best is trial 0 with value: 0.20063017308712006.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 41.5 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 38.5 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.7 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 6.6 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 6.6 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 6.6 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 6.6 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 13.1 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 13.1 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.3 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 80     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 8.2 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.1 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.4 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 6.6 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.4 K  | train\n",
      "20 | output_layer                       | Linear                          | 41     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "175 K     Trainable params\n",
      "0         Non-trainable params\n",
      "175 K     Total params\n",
      "0.702     Total estimated model params size (MB)\n",
      "732       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-17 01:29:18,211] Trial 3 finished with value: 0.17097792029380798 and parameters: {'hidden_size': 40, 'lstm_layers': 1, 'dropout': 0.05698786555311375, 'attention_head_size': 4, 'learning_rate': 0.0017647841976014306}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 35.3 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.5 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 34.6 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 8.6 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.8 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.8 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.8 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.8 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 9.5 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 9.5 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.4 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 68     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 6.0 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 2.8 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.4 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.8 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.4 K  | train\n",
      "20 | output_layer                       | Linear                          | 35     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.565     Total estimated model params size (MB)\n",
      "732       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-17 03:27:45,592] Trial 4 finished with value: 0.32790130376815796 and parameters: {'hidden_size': 34, 'lstm_layers': 1, 'dropout': 0.11778842452366008, 'attention_head_size': 4, 'learning_rate': 0.0001271180756273476}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 49.8 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.6 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 43.7 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 11.3 K | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 9.5 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 9.5 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 9.5 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 9.5 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 18.8 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 18.8 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 4.7 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 96     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 11.8 K | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 9.4 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 4.8 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 9.5 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 4.8 K  | train\n",
      "20 | output_layer                       | Linear                          | 49     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "230 K     Trainable params\n",
      "0         Non-trainable params\n",
      "230 K     Total params\n",
      "0.920     Total estimated model params size (MB)\n",
      "726       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-17 05:23:20,938] Trial 5 finished with value: 0.20375722646713257 and parameters: {'hidden_size': 48, 'lstm_layers': 1, 'dropout': 0.20230079348484636, 'attention_head_size': 1, 'learning_rate': 0.0013125322456821755}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 46.7 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.3 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 41.7 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 10.7 K | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 8.4 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 8.4 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 8.4 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 8.4 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 33.1 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 33.1 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 4.1 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 90     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 10.4 K | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 8.2 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 4.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 8.4 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 4.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 46     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "242 K     Trainable params\n",
      "0         Non-trainable params\n",
      "242 K     Total params\n",
      "0.971     Total estimated model params size (MB)\n",
      "726       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-17 07:30:35,929] Trial 6 finished with value: 0.18930679559707642 and parameters: {'hidden_size': 45, 'lstm_layers': 2, 'dropout': 0.15319910712627108, 'attention_head_size': 1, 'learning_rate': 0.0018196132071516577}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 42.6 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.0 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 39.1 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.9 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 7.0 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 7.0 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 7.0 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 7.0 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 27.6 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 27.6 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.4 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 82     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 8.7 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 5.0 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.5 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 7.0 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.5 K  | train\n",
      "20 | output_layer                       | Linear                          | 42     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "209 K     Trainable params\n",
      "0         Non-trainable params\n",
      "209 K     Total params\n",
      "0.839     Total estimated model params size (MB)\n",
      "728       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-17 09:34:23,590] Trial 7 finished with value: 0.23118767142295837 and parameters: {'hidden_size': 41, 'lstm_layers': 2, 'dropout': 0.06083710018205919, 'attention_head_size': 2, 'learning_rate': 0.0001967719421147823}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 40.5 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 37.9 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.5 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 6.3 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 6.3 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 6.3 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 6.3 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 12.5 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 12.5 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.1 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 78     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 7.8 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 6.2 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 6.3 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 40     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "171 K     Trainable params\n",
      "0         Non-trainable params\n",
      "171 K     Total params\n",
      "0.687     Total estimated model params size (MB)\n",
      "726       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-17 11:34:53,213] Trial 8 finished with value: 0.19143763184547424 and parameters: {'hidden_size': 39, 'lstm_layers': 1, 'dropout': 0.1683251888154385, 'attention_head_size': 1, 'learning_rate': 0.0012070091356097508}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 49.8 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.6 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 43.7 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 11.3 K | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 9.5 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 9.5 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 9.5 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 9.5 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 37.6 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 37.6 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 4.7 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 96     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 11.8 K | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 5.9 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 4.8 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 9.5 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 4.8 K  | train\n",
      "20 | output_layer                       | Linear                          | 49     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "264 K     Trainable params\n",
      "0         Non-trainable params\n",
      "264 K     Total params\n",
      "1.057     Total estimated model params size (MB)\n",
      "732       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-17 13:40:04,762] Trial 9 finished with value: 0.19174747169017792 and parameters: {'hidden_size': 48, 'lstm_layers': 2, 'dropout': 0.2049699538399889, 'attention_head_size': 4, 'learning_rate': 0.0004668933531340202}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 34.3 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 34.0 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 8.4 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.6 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.6 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.6 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.6 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 9.0 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 9.0 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.2 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 66     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.6 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 2.7 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.3 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.6 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.3 K  | train\n",
      "20 | output_layer                       | Linear                          | 34     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "135 K     Trainable params\n",
      "0         Non-trainable params\n",
      "135 K     Total params\n",
      "0.544     Total estimated model params size (MB)\n",
      "732       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-17 15:37:52,336] Trial 10 finished with value: 0.18160152435302734 and parameters: {'hidden_size': 33, 'lstm_layers': 1, 'dropout': 0.05435700155354668, 'attention_head_size': 4, 'learning_rate': 0.0007679455371705575}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 33.2 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.3 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 33.3 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 8.2 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 8.4 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 8.4 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 2.6 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 33     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "130 K     Trainable params\n",
      "0         Non-trainable params\n",
      "130 K     Total params\n",
      "0.523     Total estimated model params size (MB)\n",
      "732       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-17 17:22:34,228] Trial 11 finished with value: 0.185434490442276 and parameters: {'hidden_size': 32, 'lstm_layers': 1, 'dropout': 0.05006158701874641, 'attention_head_size': 4, 'learning_rate': 0.0007548523069778524}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 37.4 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.6 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 35.9 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.0 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 5.4 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 5.4 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 5.4 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 5.4 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 10.7 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 10.7 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.7 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 72     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 6.7 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.3 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.7 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 5.4 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.7 K  | train\n",
      "20 | output_layer                       | Linear                          | 37     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "152 K     Trainable params\n",
      "0         Non-trainable params\n",
      "152 K     Total params\n",
      "0.609     Total estimated model params size (MB)\n",
      "732       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-17 19:16:49,240] Trial 12 finished with value: 0.17858967185020447 and parameters: {'hidden_size': 36, 'lstm_layers': 1, 'dropout': 0.08843106116946818, 'attention_head_size': 4, 'learning_rate': 0.0008101333448341263}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 37.4 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.6 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 35.9 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.0 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 5.4 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 5.4 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 5.4 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 5.4 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 10.7 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 10.7 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.7 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 72     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 6.7 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.3 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.7 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 5.4 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.7 K  | train\n",
      "20 | output_layer                       | Linear                          | 37     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "152 K     Trainable params\n",
      "0         Non-trainable params\n",
      "152 K     Total params\n",
      "0.609     Total estimated model params size (MB)\n",
      "732       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-17 21:18:06,342] Trial 13 finished with value: 0.19565437734127045 and parameters: {'hidden_size': 36, 'lstm_layers': 1, 'dropout': 0.09783769696937714, 'attention_head_size': 4, 'learning_rate': 0.0019820670785538103}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 44.6 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.2 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 40.4 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 10.3 K | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 7.7 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 7.7 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 7.7 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 7.7 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 15.1 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 15.1 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.8 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 86     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 9.5 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.4 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.9 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 7.7 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.9 K  | train\n",
      "20 | output_layer                       | Linear                          | 44     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "193 K     Trainable params\n",
      "0         Non-trainable params\n",
      "193 K     Total params\n",
      "0.774     Total estimated model params size (MB)\n",
      "732       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-17 23:18:56,410] Trial 14 finished with value: 0.19236521422863007 and parameters: {'hidden_size': 43, 'lstm_layers': 1, 'dropout': 0.08446343922462127, 'attention_head_size': 4, 'learning_rate': 0.0008026910194317001}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 37.4 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.6 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 35.9 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.0 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 5.4 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 5.4 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 5.4 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 5.4 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 10.7 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 10.7 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.7 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 72     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 6.7 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.0 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.7 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 5.4 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.7 K  | train\n",
      "20 | output_layer                       | Linear                          | 37     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "152 K     Trainable params\n",
      "0         Non-trainable params\n",
      "152 K     Total params\n",
      "0.612     Total estimated model params size (MB)\n",
      "728       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-18 01:16:32,865] Trial 15 finished with value: 0.1808685064315796 and parameters: {'hidden_size': 36, 'lstm_layers': 1, 'dropout': 0.12736173568254897, 'attention_head_size': 2, 'learning_rate': 0.0012303435554699934}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 42.6 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.0 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 39.1 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.9 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 7.0 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 7.0 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 7.0 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 7.0 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 13.8 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 13.8 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.4 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 82     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 8.7 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.2 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.5 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 7.0 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.5 K  | train\n",
      "20 | output_layer                       | Linear                          | 42     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "181 K     Trainable params\n",
      "0         Non-trainable params\n",
      "181 K     Total params\n",
      "0.726     Total estimated model params size (MB)\n",
      "732       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-18 03:23:05,169] Trial 16 finished with value: 0.1955762654542923 and parameters: {'hidden_size': 41, 'lstm_layers': 1, 'dropout': 0.07798117113002757, 'attention_head_size': 4, 'learning_rate': 0.0006378560597855816}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 39.5 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.8 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 37.2 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.4 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 6.0 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 6.0 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 6.0 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 6.0 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 11.9 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 11.9 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.0 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 76     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 7.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.5 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.0 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 6.0 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.0 K  | train\n",
      "20 | output_layer                       | Linear                          | 39     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "163 K     Trainable params\n",
      "0         Non-trainable params\n",
      "163 K     Total params\n",
      "0.654     Total estimated model params size (MB)\n",
      "732       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-18 05:26:56,218] Trial 17 finished with value: 0.1839817464351654 and parameters: {'hidden_size': 38, 'lstm_layers': 1, 'dropout': 0.12236187792214098, 'attention_head_size': 4, 'learning_rate': 0.0011381485883841594}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 36.3 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.5 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 35.3 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 8.8 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 5.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 5.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 5.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 5.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 10.1 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 10.1 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.5 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 70     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 6.3 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 5.0 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.6 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 5.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.6 K  | train\n",
      "20 | output_layer                       | Linear                          | 36     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "148 K     Trainable params\n",
      "0         Non-trainable params\n",
      "148 K     Total params\n",
      "0.595     Total estimated model params size (MB)\n",
      "726       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-18 07:24:47,307] Trial 18 finished with value: 0.24526570737361908 and parameters: {'hidden_size': 35, 'lstm_layers': 1, 'dropout': 0.07409196425419014, 'attention_head_size': 1, 'learning_rate': 0.00026190135092942954}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 44.6 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.2 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 40.4 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 10.3 K | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 7.7 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 7.7 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 7.7 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 7.7 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 15.1 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 15.1 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.8 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 86     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 9.5 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 5.5 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.9 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 7.7 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.9 K  | train\n",
      "20 | output_layer                       | Linear                          | 44     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "194 K     Trainable params\n",
      "0         Non-trainable params\n",
      "194 K     Total params\n",
      "0.779     Total estimated model params size (MB)\n",
      "728       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-18 09:26:36,195] Trial 19 finished with value: 0.18743573129177094 and parameters: {'hidden_size': 43, 'lstm_layers': 1, 'dropout': 0.10663938252354735, 'attention_head_size': 2, 'learning_rate': 0.0005543958577072372}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 42.6 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.0 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 39.1 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.9 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 7.0 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 7.0 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 7.0 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 7.0 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 13.8 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 13.8 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.4 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 82     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 8.7 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.2 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.5 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 7.0 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.5 K  | train\n",
      "20 | output_layer                       | Linear                          | 42     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "181 K     Trainable params\n",
      "0         Non-trainable params\n",
      "181 K     Total params\n",
      "0.726     Total estimated model params size (MB)\n",
      "732       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-18 11:29:58,684] Trial 20 finished with value: 0.18510425090789795 and parameters: {'hidden_size': 41, 'lstm_layers': 1, 'dropout': 0.14059433939579788, 'attention_head_size': 4, 'learning_rate': 0.0015795158751427348}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 37.4 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.6 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 35.9 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.0 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 5.4 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 5.4 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 5.4 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 5.4 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 10.7 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 10.7 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.7 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 72     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 6.7 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.0 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.7 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 5.4 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.7 K  | train\n",
      "20 | output_layer                       | Linear                          | 37     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "152 K     Trainable params\n",
      "0         Non-trainable params\n",
      "152 K     Total params\n",
      "0.612     Total estimated model params size (MB)\n",
      "728       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-18 13:30:40,179] Trial 21 finished with value: 0.2004651427268982 and parameters: {'hidden_size': 36, 'lstm_layers': 1, 'dropout': 0.13290549297889687, 'attention_head_size': 2, 'learning_rate': 0.0010290978660099993}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 38.4 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.7 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 36.6 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.2 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 5.7 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 5.7 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 5.7 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 5.7 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 11.2 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 11.2 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.8 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 74     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 7.1 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.1 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.9 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 5.7 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.9 K  | train\n",
      "20 | output_layer                       | Linear                          | 38     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "158 K     Trainable params\n",
      "0         Non-trainable params\n",
      "158 K     Total params\n",
      "0.634     Total estimated model params size (MB)\n",
      "728       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-18 15:34:05,983] Trial 22 finished with value: 0.1859632283449173 and parameters: {'hidden_size': 37, 'lstm_layers': 1, 'dropout': 0.07027873381170383, 'attention_head_size': 2, 'learning_rate': 0.0009701355757804179}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 35.3 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.5 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 34.6 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 8.6 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.8 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.8 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.8 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.8 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 9.5 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 9.5 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.4 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 68     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 6.0 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.6 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.4 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.8 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.4 K  | train\n",
      "20 | output_layer                       | Linear                          | 35     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.568     Total estimated model params size (MB)\n",
      "728       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-18 17:31:13,932] Trial 23 finished with value: 0.1947973370552063 and parameters: {'hidden_size': 34, 'lstm_layers': 1, 'dropout': 0.09162143839098094, 'attention_head_size': 2, 'learning_rate': 0.001454856452734668}. Best is trial 3 with value: 0.17097792029380798.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 40.5 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 37.9 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.5 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 6.3 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 6.3 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 6.3 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 6.3 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 12.5 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 12.5 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.1 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 78     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 7.8 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.5 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 6.3 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 40     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "170 K     Trainable params\n",
      "0         Non-trainable params\n",
      "170 K     Total params\n",
      "0.681     Total estimated model params size (MB)\n",
      "728       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "[I 2025-11-18 19:38:21,952] Trial 24 finished with value: 0.19562789797782898 and parameters: {'hidden_size': 39, 'lstm_layers': 1, 'dropout': 0.1637826829711011, 'attention_head_size': 2, 'learning_rate': 0.0008876170497713137}. Best is trial 3 with value: 0.17097792029380798.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BEST OPTUNA PARAMS ===\n",
      "{'hidden_size': 40, 'lstm_layers': 1, 'dropout': 0.05698786555311375, 'attention_head_size': 4, 'learning_rate': 0.0017647841976014306}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "\n",
    "def objective(trial):\n",
    "    p = {\n",
    "        \"hidden_size\"        : trial.suggest_int(\"hidden_size\", best_rs[\"hidden_size\"]-8, best_rs[\"hidden_size\"]+8),\n",
    "        \"lstm_layers\"        : trial.suggest_int(\"lstm_layers\", 1, 2),\n",
    "        \"dropout\"            : trial.suggest_float(\"dropout\", 0.05, 0.25),\n",
    "        \"attention_head_size\": trial.suggest_categorical(\"attention_head_size\", [1, 2, 4]),\n",
    "        \"learning_rate\"      : trial.suggest_float(\"learning_rate\", 1e-4, 2e-3, log=True),\n",
    "    }\n",
    "\n",
    "    model = TemporalFusionTransformer.from_dataset(\n",
    "        training_ds,\n",
    "        hidden_size=p[\"hidden_size\"],\n",
    "        lstm_layers=p[\"lstm_layers\"],\n",
    "        dropout=p[\"dropout\"],\n",
    "        attention_head_size=p[\"attention_head_size\"],\n",
    "        learning_rate=p[\"learning_rate\"],\n",
    "        loss=loss_q,\n",
    "        output_size=1,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=25,\n",
    "        accelerator=\"cpu\",\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", patience=4)],\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=25)\n",
    "\n",
    "final_params = study.best_params\n",
    "print(\"\\n=== BEST OPTUNA PARAMS ===\")\n",
    "print(final_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5092db31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 41.5 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 480    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.9 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 38.5 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 9.7 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 6.6 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 6.6 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 6.6 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 6.6 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 13.1 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 13.1 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.3 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 80     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 8.2 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.1 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.4 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 6.6 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.4 K  | train\n",
      "20 | output_layer                       | Linear                          | 41     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "175 K     Trainable params\n",
      "0         Non-trainable params\n",
      "175 K     Total params\n",
      "0.702     Total estimated model params size (MB)\n",
      "732       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1085/1085 [04:58<00:00,  3.64it/s, v_num=89, train_loss_step=0.123, val_loss=0.160, train_loss_epoch=0.173] \n",
      "Best checkpoint: D:\\C14220255\\tft_checkpoints_full\\tft_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    dirpath=\"tft_checkpoints_full\",\n",
    "    filename=\"tft_final\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "early_cb = EarlyStopping(monitor=\"val_loss\", patience=6, mode=\"min\")\n",
    "\n",
    "final_model = TemporalFusionTransformer.from_dataset(\n",
    "    training_ds,\n",
    "    hidden_size=final_params[\"hidden_size\"],\n",
    "    lstm_layers=final_params[\"lstm_layers\"],\n",
    "    dropout=final_params[\"dropout\"],\n",
    "    attention_head_size=final_params[\"attention_head_size\"],\n",
    "    learning_rate=final_params[\"learning_rate\"],\n",
    "    loss=loss_q,\n",
    "    output_size=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=60,\n",
    "    accelerator=\"cpu\",\n",
    "    callbacks=[checkpoint_cb, early_cb],\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "trainer.fit(final_model, train_loader, val_loader)\n",
    "\n",
    "print(\"Best checkpoint:\", checkpoint_cb.best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82b4481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "d:\\C14220255\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pred shape: (22705, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "d:\\C14220255\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full pred shape: (22750, 9)\n",
      "Test pred shape: (45, 9)\n",
      "\n",
      "Jumlah seri yang punya periode test:\n",
      "  cabang          sku  n_test\n",
      "0    02A   BUVW001KSW       5\n",
      "1    05A   BUVW001KSW       5\n",
      "2    13A  DOPQ001K002       5\n",
      "3    13I   BUVW001KSW       5\n",
      "4    14A   BUVW001KSW       5\n",
      "5    16C  DOPQ001K009       5\n",
      "6    17A  DOPQ001K002       5\n",
      "7    23A   BUVW001KSW       5\n",
      "8    29A   BUVW001KSW       5\n",
      "\n",
      "Global metrics:\n",
      "   RMSE_train  MAE_train  MAPE%_train  sMAPE%_train     MSE_train  \\\n",
      "0  105.728947   22.74733    38.105686     58.933578  11178.610282   \n",
      "\n",
      "     RMSE_test     MAE_test  MAPE%_test  sMAPE%_test      MSE_test  \n",
      "0  2169.523608  1368.443838   41.529732    37.248821  4.706833e+06  \n",
      "\n",
      "Metrics per seri (test):\n",
      "  cabang          sku  n_test      MSE_test    RMSE_test     MAE_test  \\\n",
      "0    02A   BUVW001KSW       5  5.025600e+06  2241.784921  1943.498682   \n",
      "1    05A   BUVW001KSW       5  4.803083e+05   693.042748   459.134521   \n",
      "2    13A  DOPQ001K002       5  3.821433e+06  1954.848594  1632.913818   \n",
      "3    13I   BUVW001KSW       5  2.269858e+07  4764.302809  3739.235059   \n",
      "4    14A   BUVW001KSW       5  4.956482e+06  2226.315864  1269.267969   \n",
      "5    16C  DOPQ001K009       5  1.359452e+06  1165.955431   779.451184   \n",
      "6    17A  DOPQ001K002       5  1.637266e+05   404.631408   318.816602   \n",
      "7    23A   BUVW001KSW       5  2.114245e+06  1454.044495  1019.994531   \n",
      "8    29A   BUVW001KSW       5  1.741666e+06  1319.721805  1153.682178   \n",
      "\n",
      "   MAPE%_test  sMAPE%_test  \n",
      "0   73.025917    61.543823  \n",
      "1    9.969841    10.575310  \n",
      "2   49.281857    43.180603  \n",
      "3   50.407641    50.116257  \n",
      "4   22.860569    28.668963  \n",
      "5   49.104468    53.354321  \n",
      "6   30.170553    23.790621  \n",
      "7   30.780976    24.071390  \n",
      "8   58.165771    39.938098  \n",
      "\n",
      "Saved:\n",
      " - D:\\C14220255\\tft_full_eval\\tft_full_train_predictions.csv\n",
      " - D:\\C14220255\\tft_full_eval\\tft_full_test_predictions.csv\n",
      " - D:\\C14220255\\tft_full_eval\\tft_full_metrics_summary.csv\n",
      " - D:\\C14220255\\tft_full_eval\\tft_full_metrics_by_series.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path(r\"D:\\C14220255\")  \n",
    "OUT_DIR = PROJECT_ROOT / \"tft_full_eval\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# 4.1 Helper metric di level qty (bukan log)\n",
    "def calc_metrics_level(dfm: pd.DataFrame):\n",
    "    dfm = dfm.dropna(subset=[\"qty\", \"qty_pred\"]).copy()\n",
    "\n",
    "    err = dfm[\"qty_pred\"] - dfm[\"qty\"]\n",
    "\n",
    "    mse  = float(np.mean(err**2))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mae  = float(np.mean(np.abs(err)))\n",
    "\n",
    "    nonzero = dfm[\"qty\"] != 0\n",
    "    if nonzero.sum() > 0:\n",
    "        mape = float(\n",
    "            np.mean(\n",
    "                np.abs(dfm.loc[nonzero, \"qty_pred\"] - dfm.loc[nonzero, \"qty\"])\n",
    "                / dfm.loc[nonzero, \"qty\"]\n",
    "            ) * 100\n",
    "        )\n",
    "    else:\n",
    "        mape = np.nan\n",
    "\n",
    "    smape = float(\n",
    "        np.mean(\n",
    "            2 * np.abs(dfm[\"qty_pred\"] - dfm[\"qty\"])\n",
    "            / (np.abs(dfm[\"qty_pred\"]) + np.abs(dfm[\"qty\"]) + 1e-9)\n",
    "        ) * 100\n",
    "    )\n",
    "\n",
    "    return rmse, mae, mape, smape, mse\n",
    "\n",
    "# 4.2 Prediksi TRAIN (pakai training_ds)\n",
    "#     output di log, balik ke level dengan expm1\n",
    "train_raw = final_model.predict(training_ds, return_index=True)\n",
    "y_log_pred_train = train_raw.output.squeeze(-1)\n",
    "idx_train = train_raw.index\n",
    "\n",
    "df_train_pred = pd.DataFrame({\n",
    "    \"cabang\":   idx_train[\"cabang\"],\n",
    "    \"sku\":      idx_train[\"sku\"],\n",
    "    \"time_idx\": idx_train[\"time_idx\"],\n",
    "    \"qty_log_pred\": y_log_pred_train,\n",
    "})\n",
    "\n",
    "# merge dengan info asli (qty, periode, is_train)\n",
    "df_train_pred = df_train_pred.merge(\n",
    "    df[[\"cabang\",\"sku\",\"time_idx\",\"qty\",\"periode\",\"is_train\",\"is_test\"]],\n",
    "    on=[\"cabang\",\"sku\",\"time_idx\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# collapse duplikat window -> rata-rata per (seri, time_idx, periode)\n",
    "df_train_pred = (\n",
    "    df_train_pred\n",
    "    .groupby([\"cabang\",\"sku\",\"time_idx\",\"periode\"], as_index=False)\n",
    "    .agg(\n",
    "        qty_log_pred=(\"qty_log_pred\", \"mean\"),\n",
    "        qty=(\"qty\", \"first\"),\n",
    "        is_train=(\"is_train\", \"max\"),\n",
    "        is_test=(\"is_test\", \"max\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# balik ke level\n",
    "df_train_pred[\"qty_pred\"] = np.expm1(df_train_pred[\"qty_log_pred\"])\n",
    "\n",
    "# keep hanya baris train yang beneran train\n",
    "df_train_pred = df_train_pred[df_train_pred[\"is_train\"] == 1].copy()\n",
    "\n",
    "print(\"Train pred shape:\", df_train_pred.shape)\n",
    "\n",
    "# 4.3 Prediksi FULL (train+val+test) dari full df\n",
    "full_ds = TimeSeriesDataSet.from_dataset(\n",
    "    training_ds,\n",
    "    df,\n",
    "    stop_randomization=True\n",
    ")\n",
    "\n",
    "full_raw = final_model.predict(full_ds, return_index=True)\n",
    "y_log_pred_full = full_raw.output.squeeze(-1)\n",
    "idx_full = full_raw.index\n",
    "\n",
    "df_pred_all = pd.DataFrame({\n",
    "    \"cabang\":   idx_full[\"cabang\"],\n",
    "    \"sku\":      idx_full[\"sku\"],\n",
    "    \"time_idx\": idx_full[\"time_idx\"],\n",
    "    \"qty_log_pred\": y_log_pred_full,\n",
    "})\n",
    "\n",
    "df_pred_all = df_pred_all.merge(\n",
    "    df[[\"cabang\",\"sku\",\"time_idx\",\"qty\",\"periode\",\"is_train\",\"is_test\"]],\n",
    "    on=[\"cabang\",\"sku\",\"time_idx\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# collapse window duplicate\n",
    "df_pred_all = (\n",
    "    df_pred_all\n",
    "    .groupby([\"cabang\",\"sku\",\"time_idx\",\"periode\"], as_index=False)\n",
    "    .agg(\n",
    "        qty_log_pred=(\"qty_log_pred\", \"mean\"),\n",
    "        qty=(\"qty\", \"first\"),\n",
    "        is_train=(\"is_train\", \"max\"),\n",
    "        is_test=(\"is_test\", \"max\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_pred_all[\"qty_pred\"] = np.expm1(df_pred_all[\"qty_log_pred\"])\n",
    "\n",
    "print(\"Full pred shape:\", df_pred_all.shape)\n",
    "\n",
    "# 4.4 Filter hanya periode test (Junâ€“Okt 2024)\n",
    "EVAL_START = pd.Timestamp(\"2024-06-01\")\n",
    "EVAL_END   = pd.Timestamp(\"2024-10-01\")\n",
    "\n",
    "df_test_pred = df_pred_all[\n",
    "    (df_pred_all[\"is_test\"] == 1) &\n",
    "    (df_pred_all[\"periode\"] >= EVAL_START) &\n",
    "    (df_pred_all[\"periode\"] <= EVAL_END)\n",
    "].copy()\n",
    "\n",
    "print(\"Test pred shape:\", df_test_pred.shape)\n",
    "print(\"\\nJumlah seri yang punya periode test:\")\n",
    "print(\n",
    "    df_test_pred.groupby([\"cabang\",\"sku\"])[\"periode\"]\n",
    "                .nunique()\n",
    "                .reset_index(name=\"n_test\")\n",
    ")\n",
    "\n",
    "\n",
    "# 4.5 Metric GLOBAL train vs test\n",
    "rmse_tr, mae_tr, mape_tr, smape_tr, mse_tr = calc_metrics_level(df_train_pred)\n",
    "rmse_te, mae_te, mape_te, smape_te, mse_te = calc_metrics_level(df_test_pred)\n",
    "\n",
    "summary_metrics = pd.DataFrame([{\n",
    "    \"RMSE_train\": rmse_tr,\n",
    "    \"MAE_train\": mae_tr,\n",
    "    \"MAPE%_train\": mape_tr,\n",
    "    \"sMAPE%_train\": smape_tr,\n",
    "    \"MSE_train\": mse_tr,\n",
    "    \"RMSE_test\": rmse_te,\n",
    "    \"MAE_test\": mae_te,\n",
    "    \"MAPE%_test\": mape_te,\n",
    "    \"sMAPE%_test\": smape_te,\n",
    "    \"MSE_test\": mse_te,\n",
    "}])\n",
    "\n",
    "print(\"\\nGlobal metrics:\")\n",
    "print(summary_metrics)\n",
    "\n",
    "\n",
    "# 4.6 Metric per seri (hanya seri yang punya test)\n",
    "rows = []\n",
    "for (cab, sku), g in df_test_pred.groupby([\"cabang\",\"sku\"], sort=False):\n",
    "    rmse, mae, mape, smape, mse = calc_metrics_level(g)\n",
    "    rows.append({\n",
    "        \"cabang\": cab,\n",
    "        \"sku\": sku,\n",
    "        \"n_test\": len(g),\n",
    "        \"MSE_test\": mse,\n",
    "        \"RMSE_test\": rmse,\n",
    "        \"MAE_test\": mae,\n",
    "        \"MAPE%_test\": mape,\n",
    "        \"sMAPE%_test\": smape,\n",
    "    })\n",
    "\n",
    "metrics_by_series = pd.DataFrame(rows).sort_values([\"cabang\",\"sku\"]).reset_index(drop=True)\n",
    "print(\"\\nMetrics per seri (test):\")\n",
    "print(metrics_by_series)\n",
    "\n",
    "# 4.7 Save semua ke CSV\n",
    "train_path   = OUT_DIR / \"tft_full_train_predictions.csv\"\n",
    "test_path    = OUT_DIR / \"tft_full_test_predictions.csv\"\n",
    "summary_path = OUT_DIR / \"tft_full_metrics_summary.csv\"\n",
    "series_path  = OUT_DIR / \"tft_full_metrics_by_series.csv\"\n",
    "\n",
    "df_train_pred.to_csv(train_path, index=False)\n",
    "df_test_pred.to_csv(test_path, index=False)\n",
    "summary_metrics.to_csv(summary_path, index=False)\n",
    "metrics_by_series.to_csv(series_path, index=False)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" -\", train_path)\n",
    "print(\" -\", test_path)\n",
    "print(\" -\", summary_path)\n",
    "print(\" -\", series_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83bbd690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah seri test: 9\n",
      "Plot disimpan di folder: D:\\C14220255\\tft_full_eval\\plots_test_series\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PLOT_DIR = OUT_DIR / \"plots_test_series\"\n",
    "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# pastikan df_test_pred dari blok sebelumnya ada\n",
    "print(\"Jumlah seri test:\", df_test_pred[[\"cabang\",\"sku\"]].drop_duplicates().shape[0])\n",
    "\n",
    "for (cab, sku), g in df_test_pred.groupby([\"cabang\",\"sku\"], sort=False):\n",
    "    g = g.sort_values(\"periode\").copy()\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(g[\"periode\"], g[\"qty\"], marker=\"o\", label=\"Actual\")\n",
    "    plt.plot(g[\"periode\"], g[\"qty_pred\"], marker=\"o\", label=\"Predicted\")\n",
    "\n",
    "    plt.title(f\"{cab} - {sku}\\nPeriode test (Junâ€“Okt 2024)\")\n",
    "    plt.xlabel(\"Periode\")\n",
    "    plt.ylabel(\"Qty\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fname = f\"{cab}_{sku}_test_actual_vs_pred.png\"\n",
    "    plt.savefig(PLOT_DIR / fname, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(\"Plot disimpan di folder:\", PLOT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
